---
title: "데이터프레임과 SQL"
teaching: 180
exercises: 0
questions:
- "데이터베이스 언어 SQL을 들어보셨나요?"
- "데이터프레임을 어떻게 조작할 수 있을까?"
objectives:
- "행 혹은 칼럼을 추가하고 제거한다."
- "`NA` 값을 갖는 행을 제거한다."
- "데이터프레임 두개를 붙인다."
- "`요인(factor)`가 무엇인지 이해한다."
- "요인형 벡터를 문자형 벡터로 변환시킨다. 역으로도 수행해본다."
- "칼럼의 크기, 클래스, 칼럼명, 첫 몇 행을 포함하는 데이터프레임 기본 특성을 출력한다."
keypoints:
- "`cbind()` 함수를 사용해서 데이터프레임에 칼럼을 추가한다."
- "`rbind()` 함수를 사용해서 데이터프레임에 행을 추가한다."
- "데이터프레임에서 행을 제거한다."
- "`na.omit()` 명령문을 사용해서, `NA` 값을 갖는 행을 데이터프레임에서 제거한다."
- "`levels()`과 `as.character()` 함수를 사용해서 요인을 타맥하고 조작한다."
- "`str()`, `nrow()`, `ncol()`, `dim()`, `colnames()`, `rownames()`, `head()`, `typeof()` 함수를 사용해서 데이터프레임 구조를 파악한다."
- "`read.csv()` 함수를 사용해서 CSV 파일을 불러온다."
- "데이터프레임 `length()`가 나타내는 것이 무엇인지 이해한다."
source: Rmd
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
```

R의 모든 기본 자료형과 자료구조에 대한 여행을 마쳤다.
여러분이 수행하는 모든 작업은 이러한 도구를 조작하는 것이 된다.
하지만, 거의 대부분 쇼의 진정한 스타는 데이터프레임이다 - `csv` 파일에 정보를 불러와서 생성시킨 테이블.

`csv`, `tsv` 파일과 같은 파일을 R이나 다른 데이터베이스 시스템에서 관계형 데이터(Relational Data)로 많이 다룬다. 장점은 데이터가 일부 변형되었을 때 전부를 변경시킬 필요가 없이 변경된 일부분만 바꾼다는 점에서 큰 장점이 있다. R에서 데이터 과학자로서 혹은 데이터 분석가로서 많이 다루는 데이터가 관계형 데이터 데이터프레임이다. 

관계형 데이터 크기가 작은 경우 PC에서 데이터프레임으로 처리하면 되는데 데이터 크기가 PC 메모리 크기를 넘어가는 경우 외부 데이터베이스에 데이터를 저장하고 이를 쿼리해서 처리하는 방식을 많이 취한다. 최근에는 이를 더 확장한 개념의 하둡/스파크와 같은 NoSQL 형태 데이터도 많이 다뤄지고 있다.

<img src="../fig/dplyr-sql.png" alt="dplyr + sql" width="77%" />

# 데이터프레임(데이터프레임) 

## 데이터프레임에 행과 열을 추가하기 

데이터프레임의 칼럼은 벡터라는 것을 배웠다.
따라서, 데이터는 칼럼에서 자료형의 일관성을 유지해야 한다.
이를테면, 칼럼을 새로 추가하려면 벡터를 새로 만들어서 시작한다:


```{r, echo = FALSE}
cats <- read.csv("data/feline-data.csv")
```

```{r}
age <- c(2, 3, 5)
cats
```

`age`를 칼럼으로 다음과 같이 추가한다:


```{r}
cbind(cats, age)
```

데이터프레임의 행의 갯수와 다른 갯수를 갖는 `age` 벡터를 추가하게되면 추가되지 않고 오류가 발생됨에 주의한다:

```{r, error=TRUE}
age <- c(2, 3, 5, 12)
cbind(cats, age)

age <- c(2, 3)
cbind(cats, age)
```

왜 정상동작이 되지 않을까? 
R은 테이블의 모든 행마다, 신규 칼럼에서도 원소 하나가 있길 원한다:

```{r}
nrow(cats)
length(age)
```

그래서, 정상 동작하려면 `nrow(cats)` = `length(age)`이 되어야 한다.
`cats` 콘텐츠를 새로운 데이터프레임으로 덮어써보자.

```{r}
age <- c(2, 3, 5)
cats <- cbind(cats, age)
```

이제 행을 추가하면 어떻게 될까?
이미 데이터프레임의 행이 리스트라는 사실을 알고 있다:

```{r}
newRow <- list("tortoiseshell", 3.3, TRUE, 9)
cats <- rbind(cats, newRow)
```

## 요인 (Factors)

살펴볼 것이 하나더 있다: `요인(factor)`에서 각기 다른 값을 `수준(level)`이라고 한다.
요인형 "coat" 변수는 수준이 3으로 구성된다: "black", "calico", "tabby". 
R은 세가지 수준 중 하나와 매칭되는 값만 받아들인다.
완전 새로운 값을 추가하게 되면, 추가되는 신규 값은 `NA`가 된다.

경고 메시지를 통해서 *coat* 요인변수에 "tortoiseshell" 값을 추가하는데 성공하지 못했다고 알려준다.
하지만, 3.3 (숫자형), TRUE (논리형), and 9 (숫자형) 모두 *weight*, *likes_string*, *age* 변수에 
성공적으로 추가된다. 왜냐하면 변수가 요인형이 아니라서 그렇다.
"tortoiseshell"을 *coat* 요인변수에 성공적으로 추가하려면, 요인의 수준(level)로 "tortoiseshell"을 추가하면 된다:

```{r}
levels(cats$coat)
levels(cats$coat) <- c(levels(cats$coat), "tortoiseshell")
cats <- rbind(cats, list("tortoiseshell", 3.3, TRUE, 9))
```

대안으로, 요인형 벡터를 문자형 벡터로 변환시키면 된다;
요인변수의 범주를 잃게 되지만, 요인 수준을 조심스럽게 다룰 필요없이, 
칼럼에 추가하고자 하는 임의 단어를 추가하면 된다:

```{r}
str(cats)
cats$coat <- as.character(cats$coat)
str(cats)
```

> ## 도전과제 1
> 
> 1. `cats$age` 벡터에 7을 곱해서 `human_age` 벡터를 생성하자.
> 2. `human_age`를 요인형으로 변환시키자.
> 3. `as.numeric()` 함수를 사용해서 `human_age` 벡털르 다시 숫자형 벡터로 변환시킨다.
>    이제 7로 나눠서 원래 고양이 나이로 되돌리자. 무슨 일이 생겼는지 설명하자.
>
> > ## 도전과제 1에 대한 해답
> > 1. `human_age <- cats$age * 7`
> > 2. `human_age <- factor(human_age)`. `as.factor(human_age)` works just as well.
> > 3. `as.numeric(human_age)`을 실행하면 `1 2 3 4 4`이 된다.
> > 왜냐하면 요인형 변수는 정수형(여기서 1:4)으로 자료를 저장하기 때문이다. 
> > 정수 라벨과 연관된 값은 여기서 28, 35, 56, 63이다.
> > 요인형 변수를 숫자형 벡터로 변환시키면 라벨이 아니라 그 밑단의 정수를 반환시킨다.
> > 원래 숫자를 원하는 경우, `human_age`를 문자형 벡터로 변환시키고 나서 숫자형 벡터로 변환시키면 된다.(왜 이방식은 정장 동작할까?) 
> > 실수로 숫자만 담긴 칼럼 어딘가 문자가 포함된 csv 파일로 작업할 때 이런 일이 실제로 종종 일어난다.
> > 데이터를 불러 읽어올 때 `stringsAsFactors=FALSE` 설정을 잊지말자.
> {: .solution}
{: .challenge}

## 행 제거

이제 데이터프레임에 행과 열을 추가하는 방법을 알게 되었다 - 하지만, 
데이터프레임에 "tortoiseshell" 고양이를 처음으로 추가하면서, 우연히 
쓰레기 행을 추가시켰다:

```{r}
cats
```

데이터프레임에 문제가 되는 행을 마이너스해서 빼자:


```{r}
cats[-4, ]
```

`-4,` 다음에 아무것도 적시하지 않아서 4번째 행 전체를 제거함에 주목한다.

주목: 벡터 내부에 행 다수를 넣어 한번에 행을 제거할 수도 있다: `cats[c(-4,-5), ]`

대안으로, `NA` 값을 갖는 모든 행을 제거시킨다:

```{r}
na.omit(cats)
```

출력결과를 `cats`에 다시 대입하여 변경사항이 데이터프레임이 영구히 남도록 조치한다:

```{r}
cats <- na.omit(cats)
```

## 칼럼 제거

데이터프레임의 칼럼도 제거할 수 있다.
"age" 칼럼을 제거하고자 한다면 어떨까?
변수명과 변수 인덱스, 두가지 방식으로 칼럼을 제거할 수 있다.

```{r}
cats[,-4]
```

`,-4` 앞에 아무것도 없는 것에 주목한다. 모든 행을 간직한다는 의미를 갖는다.

대안으로, 색인명을 사용해서 컬럼을 제거할 수도 있다.

```{r}
drop <- names(cats) %in% c("age")
cats[,!drop]
```

## 데이터프레임에 덧붙이기

데이터프레임에 데이터를 추가시킬 때 기억할 것은 **칼럼은 벡터, 행은 리스트**라는 사실이다.
`rbind()` 함수를 사용해서 데이터프레임 두개를 본드로 붙이듯이 결합시킬 수 있다:

```{r}
cats <- rbind(cats, cats)
cats
```
But now the row names are unnecessarily complicated. We can remove the rownames,
and R will automatically re-name them sequentially:

행명칭(rownames)이 불필요하게 복잡해져, 행명칭을 제거하면,
자동적으로 R이 순차적으로 행명칭을 부여시킨다.

```{r}
rownames(cats) <- NULL
cats
```

> ## 도전과제 2
>
> 다음 구문을 사용해서 R 내부에서 직접 데이터프레임을 새로 만들 수 있다:
> ```{r}
> df <- data.frame(id = c("a", "b", "c"),
>                  x = 1:3,
>                  y = c(TRUE, TRUE, FALSE),
>                  stringsAsFactors = FALSE)
> ```
> 다음 정보를 갖는 데이터프레임을 직접 제작해 보자:
>
> - 이름(first name)
> - 성(last name)
> - 좋아하는 숫자
>
> `rbind`를 사용해서 옆사람을 항목에 추가한다.
>  마지막으로 `cbind()`함수를 사용해서 "지금이 커피시간인가요?"라는 질문의 답을 칼럼으로 추가한다. 
>
> > ## 도전과제 2에 대한 해답
> > ```{r}
> > df <- data.frame(first = c("Grace"),
> >                  last = c("Hopper"),
> >                  lucky_number = c(0),
> >                  stringsAsFactors = FALSE)
> > df <- rbind(df, list("Marie", "Curie", 238) )
> > df <- cbind(df, coffeetime = c(TRUE,TRUE))
> > ```
> {: .solution}
{: .challenge}

## 현실적인 예제

지금까지 고양이 데이털르 가지고 데이터프레임 조작에 대한 기본적인 사항을 살펴봤다.
이제 학습한 기술을 사용해서 좀더 현실적인 데이터셋을 다뤄보자.
앞에서 다운로드 받은 `gapminder` 데이터셋을 불러오자:

```{r}
gapminder <- read.csv("data/gapminder_data.csv")
```

> ## 기타 팁
>
> * 흔히 맞닥드리는 또다른 유형의 파일이 탭구분자를 갖는 파일(.tsv)이다. 탭을 구분자로 명세하는데, `"\\t"`을 사용하고, `read.delim()` 함수로 불러 읽어온다.
>
> * 파일을 `download.file()` 함수를 사용해서 인터넷으로부터 직접 본인 컴퓨터 폴더로 다운로드할 수 있다.
>   `read.csv()` 함수를 실행해서 다운로드 받은 파일을 읽어온다. 예를 들어,
> ```{r eval=FALSE, echo=TRUE}
> download.file("https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder_data.csv", destfile = "data/gapminder_data.csv")
> gapminder <- read.csv("data/gapminder_data.csv")
> ```
>
> * 대안으로, `read.csv()` 함수 내부에 파일 경로를 웹주소를 치환해서 인터넷에서 직접 파일을 불러올 수도 있다.
>   이런 경우 로컬 컴퓨터에 csv 파일이 전혀 저장되지 않았다는 점을 주의한다. 예를 들어,
> ```{r eval=FALSE, echo=TRUE}
> gapminder <- read.csv("https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder_data.csv")
> ```
>
> * [readxl](https://cran.r-project.org/web/packages/readxl/index.html) 팩키지를 사용해서,
>   엑셀 스프레드쉬트를 평범한 텍스트로 변환하지 않고 직접 불러올 수도 있다.
{: .callout}

`gapminder` 데이터셋을 좀더 살펴보자; 항상 가장 먼저 해야되는 작업은 
`str` 명령어로 데이터가 어떻게 생겼는지 확인하는 것이다:

```{r}
str(gapminder)
```

`typeof()` 함수로 데이터프레임 칼럼 각각을 면밀히 조사할 수도 있다:

```{r}
typeof(gapminder$year)
typeof(gapminder$country)
str(gapminder$country)
```

데이터프레임 차원에 정보를 얻어낼 수도 있다;
`str(gapminder)` 실행결과 `gapminder` 데이터프레임에 관측점 1704, 변수 6개가 있음을 상기한다.
다음 코드 실행결과는 무엇일까? 그리고 왜 그렇게 되는가?

```{r}
length(gapminder)
```

공정한 추측은 아마도 데이터프레임 길이가 행의 길이(1704)라고 보는 것이다.
하지만, 이번에는 다르다; 데이터프레임은 **벡터와 요인으로 구성된 리스트**라는 사실이다:

```{r}
typeof(gapminder)
```

`length()` 함수는 6을 제시하는데, 이유는 `gapminder`가 6개 칼럼을 갖는 리스트로 만들어졌기 때문이다.
데이터셋에서 행과 열 숫자를 얻는데 다음 함수를 던져보자:

```{r}
nrow(gapminder)
ncol(gapminder)
```

혹은 한번에 보려면:

```{r}
dim(gapminder)
```

또한, 모든 칼럼의 칼럼명이 무엇인지 파악하고자 하면 다음과 같이 질문을 던진다:


```{r}
colnames(gapminder)
```

현 단계에서, R이 제시하는 구조가 우리의 직관 혹은 예상과 부합되는지 묻어보는 것이 중요하다;
각 칼럼에 대한 기본 자료형은 이해가 되는가?
만약 납득이 가지 않는다면, 후속 작업에서 나쁜 놀라운 사실로 전환되기 전에 문제를 해결해야 한다.
문제를 해결하는데, R이 데이터를 이해하는 방법과 데이터를 기록할 때 *엄격한 일관성(strict consistency)*의 
중요성에 관해 학습한 것을 동원한다.

자료형과 자료구조가 타당해 보이게 되면, 데이터를 제대로 파고들어갈 시간이 되었다.
`gapminder` 데이터 처음 몇줄을 살펴보자:

```{r}
head(gapminder)
```

> ## 도전과제 3
>
> 데이터 마지막 몇줄, 중간 몇줄을 점검하는 것도 좋은 습관이다. 그런데 어떻게 점검할 수 있을까?
>
> 중간 몇줄을 찾아보는 것이 너무 어렵지는 않지만, 임의로 몇줄을 추출할 수도 있다. 어떻게 할 수 있을까요?
>
> > ## 도전과제 3에 대한 해답
> > 마지막 몇줄을 점검하려면, R에 내장된 함수가 있어서 상대적으로 간단하다:
> > 
> > ~~~
> > tail(gapminder)
> > tail(gapminder, n = 15)
> > ~~~
> > {: .r}
> > 
> > 데이터가 온전한지(혹은 관점에 따라 데이터가 온전하지 않은지)를 점검하는데 몇줄을 추출할 수 있을까요?
> > ## 팁: 몇가지 방법이 존재한다.
> > 중첩함수(또다른 함수에 인자로 전달되는 함수)를 사용한 해법도 있다. 
> > 새로운 개념처럼 들리지만, 사실 이미 사용하고 있다.
> > my_dataframe[rows, cols]  명령어는 데이터프레임을 화면에 뿌려준다.
> > 데이터프레임에 행이 얼마나 많은지 알지 못하는데 어떻게 마지막 행을 뽑아낼 수 있을까?
> > R에 내장된 함수가 있다.
> > (의사) 난수를 얻어보는 것은 어떨가? R은 난수추출 함수도 갖추고 있다.
> > ~~~
> > gapminder[sample(nrow(gapminder), 5), ]
> > ~~~
> > {: .r}
> {: .solution}
{: .challenge}


분석결과를 재현가능하게 확실하게 만들려면,
코드를 스크립트 파일에 저장해서 나중에 다시 볼 수 있어야 한다.

> ## 도전과제 4
>
> `file -> new file -> R script`로 가서,
> `gapminder` 데이터셋을 불러오는 R 스크립틀르 작성한다.
> `scripts/` 디렉토리에 저장하고 버전제어 시스템에도 추가한다.
>
> 인자로 파일 경로명을 사용해서 `source()` 함수를 사용해서 스크립트를 실행하라.
> (혹은 RStudio "source" 버튼을 누른다)
>
> > ## 도전과제 4에 대한 해답
> > `scripts/load-gapminder.R` 파일에 담긴 내용물은 다음과 같다:
> > ```{r eval = FALSE}
> > download.file("https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder_data.csv", destfile = "data/gapminder_data.csv")
> > gapminder <- read.csv(file = "data/gapminder_data.csv")
> > ```
> > 스크립트를 실행시키면 데이터를 `gapminder` 변수에 적재시킨다:
> > ```{r eval = FALSE}
> > source(file = "scripts/load-gapminder.R")
> > ```
> {: .solution}
{: .challenge}

> ## 도전과제 5
>
> `str(gapminder)` 출력결과를 다시 불러오자;
> 이번에는 `gapminder` 데이터에 대해 `str()` 함수가 출력하는 모든 것이 의미하는 바를 설명한다.
> 지금까지 학습한 요인, 리스트와 벡터 뿐만 아니라, `colnames()`, `dim()`와 같은 함수도 동원한다.
> 이해하지 못한 부분이 있다면, 주면 동료와 상의한다!
> > ## 도전과제 5에 대한 해답
> >
> > `gapminder` 객체는 다음 칼럼을 갖는 데이터프레임이다.
> > - `country` `continent` 변수는 요인형 벡터
> > - `year` 변수는 정수형 벡터
> > - `pop`, `lifeExp`, `gdpPercap` 변수는 숫자형 벡터
> >
> {: .solution}
{: .challenge}


# SQL 

- [소프트웨어 카펜트리 SQL 사용하기 (2015)](http://statkclee.github.io/xwmooc-sc/novice/sql/)

- [데이터 카펜트리 SQL 학습교재 (2016)](https://statkclee.github.io/sql-ecology-lesson/)

# `dplyr` + SQL [^recology-sql]

[^recology-sql]: [Data Carpentry contributors, "SQL databases and R"](https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html)

SQL과 `dplyr`을 사용해서 관계형 데이터를 데이터프레임 혹은 데이터베이스 테이블을 통해 조작하는 방식을 학습했다. 이번에는 `dplyr` 팩키지 동사를 SQL 쿼리문으로 변환시켜서 메모리에서 처리할 수 없는 큰 데이터를 데이터베이스에서 쿼리하는 방식을 `dbplyr` 팩키지를 통해서 학습해 보자.

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 10px;}
</style>
<div class = "blue">

- **학습목표**
    - R에서 데이터베이스 접속
    - `RSQLite`와 `dplyr` 팩키지 사용해서 R에서 SQL 쿼리 실행시킴
    - `.csv` 파일에서 `SQLite` 데이터베이스 생성시킴
    
</div>

# `dbplyr` 헬로우 월드 

`tidyverse`를 구성하는 [`dbplyr`](https://github.com/tidyverse/dbplyr)에 나와 있는 예제를 바탕으로 `dplyr` 동사를 데이터베이스에 던져 쿼리하는 사례를 살펴보자.


가장 먼저, 필요한 팩키지를 불러온다. `tidyverse` 팩키지를 구성하는 `dplyr` 팩키지와 데이터베이스에 연결하여 `dplyr` 동사를 던질 수 있도록 도와주는 `dbplyr` 팩키지를 준비한다.

그리고 나서 메모리에 `SQLite` 데이터베이스를 연결시킨다. 
`mtcars` 데이터프레임을 `copy_to()` 함수로 테이블로 복사해 넣는다.

`tbl()` 함수로 "mtcars" 테이블에 연결점을 만들고 `WHERE` 절 대신 `filter()` 함수로 데이터 조작 쿼리문을 만든다.


```{r dbplyr-hello-world}
# library(dplyr, warn.conflicts = FALSE)
library(tidyverse)
library(dbplyr)

con <- DBI::dbConnect(RSQLite::SQLite(), ":memory:")
copy_to(con, mtcars)

mtcars_db <- tbl(con, "mtcars")

mtcars_db %>% 
  filter(mpg >20)
```

물론 `src_dbi()` 함수로 데이터베이스에 연결된 정보를 확인할 수 있고, `sql()` 함수를 통해 직접 SQL 문을 던질 수도 있고, 이를 데이터프레임 객체로 저장해서 후속작업을 하는 것도 가능하다.

```{r dbplyr-hello-world-sql}
dbplyr::src_dbi(con)
tbl(con, sql("SELECT mpg, cyl, vs, am FROM mtcars WHERE mpg > 20"))

tbl(con, sql("SELECT mpg, cyl, vs, am FROM mtcars WHERE mpg > 20")) %>% 
  ggplot(aes(x=as.factor(am), y=mpg)) +
    geom_boxplot()
```


## 왜 데이터베이스를 사용할까?


지금까지 본인 컴퓨터 메모리에 쉽게 올려서 작업할 수 있는 작은 데이터셋을 다루었다.
하지만, 본인 컴퓨터 메모리에 올리기에는 너무 큰 데이터셋을 다루려고 한다면 어떨가요?
이런 경우, R 외부에 데이터셋을 저장시키고, 데이터베이스에서 관리하는 것이 답이 될 수 있다.
데이터베이스에 접근해서, 현재 필요한 분석 데이터 일부만 가져와서 작업하는 모형이 된다.


이런 접근법이 좋은 점은 엄청 큰 데이터셋이 이미 공공 혹은 사설 데이터베이스를 통해 활용이 가능하다.
즉, 전체 데이터를 먼저 다운로드할 필요없이 필요한 데이터만 쿼리해서 가져오면 된다.


R은 현존하는 거의 모든 데이터베이스에 접근할 수 있다. 
거의 모든 데이터베이스에 대응되는 R 팩키지가 있어 연결할 수 있다. (예를 들어, **`RSQLite`**, RMySQL, 등)
앞선 수업에서 사용한 [**`dplyr`**](https://cran.r-project.org/web/packages/dplyr/index.html) 팩키지는 
널리 사용되는 오픈 소스 데이터베이스 [sqlite](https://sqlite.org/), [mysql](https://www.mysql.com/), [postgresql](https://www.postgresql.org/), 
구글 [bigquery](https://cloud.google.com/bigquery/)를 지원할 뿐만 아니라,
`dplyr` 팩키지 [소품문(vignette)](https://cran.r-project.org/web/packages/dbplyr/vignettes/new-backend.html)에서 다른 데이터베이스 접속하는 
방법도 기술하고 있다.
RStudio사에서 [DB 웹사이트](http://db.rstudio.com/)도 만들어서 데이터베이스 인프라로 작업할 수 있는 모범 사례 뿐만 아니라 문서도 제공하고 있다.


**`dplyr`**을 사용해서 데이터베이스와 인터페이스하는 것은 `SELECT` SQL 구문을 생성해서 
데이터셋을 가져와서 분석하는데 방점을 두고 있다. 따라서 데이터베이스 자체를 변형시키지는 않는다.
**`dplyr`**은 SQL 구문이 갖고 있는 `UPDATE` 혹은 `DELETE` 구문을 제공하지 않는다.
데이터베이스 변경이 필요한 경우 다른 R 팩키지 (예를 들어, **`RSQLite`**)를 활용하면 된다.
이번 학습에서 **`dplyr`** 팩키지를 사용해서 데이터베이스와 작업하는 방법을 시연하는데 
**`dplyr`** 동사 구문과 SQL 구문을 함께 사용한다.


### `portal_mammals` 데이터베이스

이전 학습에서 친숙해진 `surveys` 데이터로 작업을 계속할 것이다.
먼저, **`dbplyr`** 팩키지를 설치하자:


```{r dbplyr-install, eval=FALSE, purl=TRUE}
install.packages(c("dbplyr", "RSQLite"))
```

SQLite 데이터베이스는 `portal_mammals.sqlite` 파일에 담겨 있어서, 아직 
갖고 있지 않다면, `Figshare` 사이트에서 `data` 하위 디렉토리 위치에서 확인할 수 있고, 다음 명령어를 사용해서 다운로드 받으면 된다:


```{r download, eval = FALSE, purl=FALSE}
dir.create("data", showWarnings = FALSE)
download.file(url = "https://ndownloader.figshare.com/files/2292171",
              destfile = "data/portal_mammals.sqlite", mode = "wb")
```

## 데이터베이스 연결

다음 명령어를 사용해서 R이 데이터베이스를 가리키도록 한다:

```{r connect, purl=TRUE}
library(dplyr)
library(dbplyr)
mammals <- DBI::dbConnect(RSQLite::SQLite(), "../data/portal_mammals.sqlite")
```

상기 명령어는 팩키지 2개를 사용해서 **`dbplyr`**, **`dplyr`** 가 협력해서 SQLite 데이터베이스와 대화가 가능하도록 한다.
**`DBI`** 팩키지는 유저로 직접 사용하는 것은 아니다.
**`DBI`** 팩키지는 데이터베이스 관리시스템(DBMS) 종류에 관계없이 R이 명령어를 전달할 수 있도록 하는 역할을 담당한다.
**`RSQLite`** 팩키지는 R이 SQLite 데이터베이스와 붙어 작업할 수 있도록 하는 역할을 수행한다.

This command does not load the data into the R session (as the
`read_csv()` function did). Instead, it merely instructs R to connect to
the `SQLite` database contained in the `portal_mammals.sqlite` file.



Using a similar approach, you could connect to many other database management systems that are supported by R including MySQL, PostgreSQL, BigQuery, etc.

Let's take a closer look at the `mammals` database we just connected to:

```{r tables, results="markup"}
src_dbi(mammals)
```

Just like a spreadsheet with multiple worksheets, a SQLite database can contain
multiple tables. In this case three of them are listed in the `tbls` row in the
output above:

* plots
* species
* surveys

Now that we know we can connect to the database, let's explore how to get
the data from its tables into R.

### Querying the database with the SQL syntax

To connect to tables within a database, you can use the `tbl()` function from
**`dplyr`**. This function can be used to send SQL queries to the database. To
demonstrate this functionality, let's select the columns "year", "species_id",
and "plot_id" from the `surveys` table:

```{r use-sql-syntax, purl=TRUE}
tbl(mammals, sql("SELECT year, species_id, plot_id FROM surveys"))
```

With this approach you can use any of the SQL queries we have seen in the
database lesson.

### Querying the database with the dplyr syntax

One of the strengths of **`dplyr`** is that the same operation can be done using
**`dplyr`**'s verbs instead of writing SQL. First, we select the table on which to do
the operations by creating the `surveys` object, and then we use the standard
**`dplyr`** syntax as if it were a data frame:

```{r use-dplyr-syntax}
surveys <- tbl(mammals, "surveys")
surveys %>%
    select(year, species_id, plot_id)
```

In this case, the `surveys` object behaves like a data frame. Several
functions that can be used with data frames can also be used on tables from a
database. For instance, the `head()` function can be used to check the first 10
rows of the table:


```{r table_details, results='show', purl=FALSE}
head(surveys, n = 10)
```

This output of the `head` command looks just like a regular `data.frame`:
The table has 9 columns and the `head()` command shows us the first 10 rows.
Note that the columns `plot_type`, `taxa`, `genus`, and `species` are missing.
These are now located in the tables `plots` and `species` which we will join
together in a moment.

However, some functions don't work quite as expected. For instance, let's check
how many rows there are in total using `nrow()`:

```{r nrows, results='show', purl=FALSE}
nrow(surveys)
```

That's strange - R doesn't know how many rows the `surveys` table contains - it
returns `NA` instead. You might have already noticed that the first line of
the `head()` output included `??` indicating that the number of rows wasn't
known.

The reason for this behavior highlights a key difference between using
**`dplyr`** on datasets in memory (e.g. loaded into your R session via
`read_csv()`) and those provided by a database. To understand it, we take a
closer look at how **`dplyr`** communicates with our SQLite database.

### SQL translation

Relational databases typically use a special-purpose language,
[Structured Query Language (SQL)](https://en.wikipedia.org/wiki/SQL),
to manage and query data.

For example, the following SQL query returns the first 10 rows from the
`surveys` table:

```sql
SELECT *
FROM `surveys`
LIMIT 10
```

Behind the scenes, **`dplyr`**:

1. translates your R code into SQL
2. submits it to the database
3. translates the database's response into an R data frame

To lift the curtain, we can use **`dplyr`**'s `show_query()` function to show which SQL
commands are actually sent to the database:

```{r show_query, message=TRUE, purl=FALSE}
show_query(head(surveys, n = 10))
```

The output shows the actual SQL query sent to the database; it matches our
manually constructed `SELECT` statement above.

Instead of having to formulate the SQL query ourselves - and
having to mentally switch back and forth between R and SQL syntax - we can
delegate this translation to **`dplyr`**. (You don't even need to know SQL to interact
with a database via **`dplyr`**!)

**`dplyr`**, in turn, doesn't do the real work of subsetting the table, either.
Instead, it merely sends the query to the database, waits for its response and
returns it to us.

That way, R never gets to see the full `surveys` table - and that's why it could
not tell us how many rows it contains. On the bright side, this allows us to work
with large datasets - even too large to fit into our computer's memory.

**`dplyr`** can translate many different query types into SQL allowing us to, e.g.,
`select()` specific columns, `filter()` rows, or join tables.

To see this in action, let's compose a few queries with **`dplyr`**.

## Simple database queries

First, let's only request rows of the `surveys` table in which `weight` is less
than 5 and keep only the species_id, sex, and weight columns.

```{r pipe, results='show', purl=FALSE}
surveys %>%
  filter(weight < 5) %>%
  select(species_id, sex, weight)
```

Executing this command will return a table with 10 rows and the requested
`species_id`, `sex` and `weight` columns. Great!

... but wait, why are there only 10 rows?

The last line:

```
# ... with more rows
```

indicates that there are more results that fit our filtering criterion. Why was
R lazy and only retrieved 10 of them?

## Laziness

Hadley Wickham, the author of **`dplyr`**
[explains](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html):

> When working with databases, **`dplyr`** tries to be as lazy as possible:
>
> * It never pulls data into R unless you explicitly ask for it.
> * It delays doing any work until the last possible moment - it collects together
> everything you want to do and then sends it to the database in one step.

When you construct a **`dplyr`** query, you can connect multiple verbs into a single
pipeline. For example, we combined the `filter()` and `select()` verbs using the
`%>%` pipe.

If we wanted to, we could add on even more steps, e.g. remove the `sex` column
in an additional `select` call:

```{r pipe2, results='show', purl=FALSE}
data_subset <- surveys %>%
  filter(weight < 5) %>%
  select(species_id, sex, weight)

data_subset %>%
  select(-sex)
```

Just like the first `select(species_id, sex, weight)` call, the `select(-sex)`
command is not executed by R. It is sent to the database instead. Only the
_final_ result is retrieved and displayed to you.

Of course, we could always add on more steps, e.g., we could filter by
`species_id` or  minimum `weight`. That's why R doesn't retrieve the full set
of results - instead it only retrieves the first 10 results from the database
by default. (After all, you might want to add an additional step and get the
database to do more work...)

To instruct R to stop being lazy, e.g. to retrieve all of the query results from
the database, we add the `collect()` command to our pipe. It indicates that our
database query is finished: time to get the _final_ results and load them into
the R session.

```{r collect, results='markup', purl=FALSE}
data_subset <- surveys %>%
  filter(weight < 5) %>%
  select(species_id, sex, weight) %>%
  collect()
```

Now we have all 17 rows that match our query in a `data.frame` and can continue
to work with them exclusively in R, without communicating with the database.

## Complex database queries

**`dplyr`** enables database queries across one or multiple database tables, using
the same single- and multiple-table verbs you encountered previously. This means
you can use the same commands regardless of whether you interact with a remote
database or local dataset! This is a really useful feature if you work with
large datasets: you can first prototype your code on a small subset that fits
into memory, and when your code is ready, you can change the input dataset to
your full database without having to change the syntax.

On the other hand, being able use SQL queries directly can be useful if your
collaborators have already put together complex queries to prepare the dataset
that you need for your analysis.

To illustrate how to use **`dplyr`** with these complex queries, we are going to join
the `plots` and `surveys` tables. The `plots` table in the database contains
information about the different plots surveyed by the researchers. To access it,
we point the `tbl()` command to it:

```{r plots, results='markup', purl=FALSE}
plots <- tbl(mammals, "plots")
plots
```

The `plot_id` column also features in the `surveys` table:

```{r surveys, results='markup', purl=FALSE}
surveys
```

Because `plot_id` is listed in both tables, we can use it to look up matching
records, and join the two tables.


![diagram illustrating inner and left joins](./img/joins.svg)


For example, to extract all surveys for the first plot, which has `plot_id` 1,
we can do:

```{r join, results='markup', purl=FALSE}
plots %>%
  filter(plot_id == 1) %>%
  inner_join(surveys) %>%
  collect()
```



**Important Note:** Without the `collect()` statement, only the first 10
matching rows are returned. By adding `collect()`, the full set of 1,985 is
retrieved.

> ### Challenge
>
> Write a query that returns the number of rodents observed in each plot in
> each year.
>
> Hint: Connect to the species table and write a query that joins the species
> and survey tables together to exclude all non-rodents.
> The query should return counts of rodents by year.
>
> Optional: Write a query in SQL that will produce the same result. You can join
> multiple tables together using the following syntax where foreign key refers
> to your unique id (e.g., `species_id`):
>
> ```sql
> SELECT table.col, table.col
> FROM table1 JOIN table2
> ON table1.key = table2.key
> JOIN table3 ON table2.key = table3.key
> ```
>
> ```{r left_join_answer, answer=TRUE, eval=TRUE}
> ## with dplyr syntax
> species <- tbl(mammals, "species")
> 
> left_join(surveys, species) %>%
>   filter(taxa == "Rodent") %>%
>   group_by(taxa, year) %>%
>   tally %>%
>   collect()
> 
> ## with SQL syntax
> query <- paste("
> SELECT a.year, b.taxa,count(*) as count
> FROM surveys a
> JOIN species b
> ON a.species_id = b.species_id
> AND b.taxa = 'Rodent'
> GROUP BY a.year, b.taxa",
> sep = "" )
> 
> tbl(mammals, sql(query))
> >
> ``` 


```{r challenge-sql-1, purl=TRUE, echo=FALSE}

### Challenge
## Write a query that returns the number of rodents observed in each
## plot in each year.

##  Hint: Connect to the species table and write a query that joins
##  the species and survey tables together to exclude all
##  non-rodents. The query should return counts of rodents by year.

## Optional: Write a query in SQL that will produce the same
## result. You can join multiple tables together using the following
## syntax where foreign key refers to your unique id (e.g.,
## `species_id`):

## SELECT table.col, table.col
## FROM table1 JOIN table2
## ON table1.key = table2.key
## JOIN table3 ON table2.key = table3.key

```

> ### Challenge
>
>  Write a query that returns the total number of rodents in each genus caught
>  in the different plot types.
>
>  Hint: Write a query that joins the species, plot, and survey tables together.
>  The query should return counts of genus by plot type.
> 
> ```{r genus_by_type_answer, answer=TRUE, results='markup', eval=FALSE}
> species <- tbl("mammals", species)
> genus_counts <- left_join(surveys, plots) %>%
>   left_join(species) %>%
>   group_by(plot_type, genus) %>%
>   tally %>%
>   collect()
> ```



```{r challenge-sql-2, purl=TRUE, echo=FALSE}
### Challenge

## Write a query that returns the total number of rodents in each
## genus caught in the different plot types.

##  Hint: Write a query that joins the species, plot, and survey
##  tables together.  The query should return counts of genus by plot
##  type.
```

This is useful if we are interested in estimating the number of individuals
belonging to each genus found in each plot type. But what if we were interested
in the number of genera found in each plot type? Using `tally()` gives the
number of individuals, instead we need to use `n_distinct()` to count the
number of unique values found in a column.

```{r count_unique_genera, purl=FALSE}
species <- tbl(mammals, "species")
unique_genera <- left_join(surveys, plots) %>%
    left_join(species) %>%
    group_by(plot_type) %>%
    summarize(
        n_genera = n_distinct(genus)
    ) %>%
    collect()
```

`n_distinct`, like the other **`dplyr`** functions we have used in this lesson, works
not only on database connections but also on regular data frames.


## Creating a new SQLite database

So far, we have used a previously prepared SQLite database. But we can also
use R to create a new database, e.g. from existing `csv` files.  Let's recreate
the mammals database that we've been working with, in R. First let's read in the
`csv` files.

```{r data_frames, purl=TRUE, eval=FALSE}
species <- read_csv("data/species.csv")
surveys <- read_csv("data/surveys.csv")
plots <- read_csv("data/plots.csv")
```

Creating a new SQLite database with **`dplyr`** is easy. You can re-use the same
command we used above to open an existing `.sqlite` file. The `create = TRUE`
argument instructs R to create a new, empty database instead.

**Caution:** When `create = TRUE` is added, any existing database at the same
location is overwritten _without warning_.

```{r create_database, purl=TRUE, eval=FALSE}
my_db_file <- "portal-database.sqlite"
my_db <- src_sqlite(my_db_file, create = TRUE)
```

Currently, our new database is empty, it doesn't contain any tables:

```{r empty, results='show', eval=FALSE}
my_db
```

To add tables, we copy the existing data.frames into the database one by one:

```{r copy, purl=FALSE, eval=FALSE}
copy_to(my_db, surveys)
copy_to(my_db, plots)
my_db
```

If you check the location of our database you'll see that data is automatically
being written to disk. R and **`dplyr`** not only provide easy ways to query
existing databases, they also allows you to easily create your own databases
from flat files!

> ### Challenge
>
> Add the remaining species table to the `my_db` database and run some of your
> queries from earlier in the lesson to verify that you have
> faithfully recreated the mammals database.


```{r challenge-sql-3, purl=TRUE, echo=FALSE}
### Challenge

## Add the remaining species table to the my_db database and run some
## of your queries from earlier in the lesson to verify that you
## have faithfully recreated the mammals database.
```

**Note:** In this example, we first loaded all of the data into the R session by
reading the three `csv` files. Because all the data has to flow through R,
this is not suitable for very large datasets.

